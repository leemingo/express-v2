{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6be2b940",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-11 23:19:44.743891: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-06-11 23:19:44.746390: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-06-11 23:19:44.776807: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-06-11 23:19:44.776840: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-06-11 23:19:44.776860: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-06-11 23:19:44.783044: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "import torch\n",
    "\n",
    "from datasets import PressingSequenceDataset, SoccerMapInputDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44ac95b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7795, 888)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = \"/data/MHL/pressing-intensity\"\n",
    "\n",
    "with open(f\"{data_path}/train_dataset.pkl\", \"rb\") as f:\n",
    "    train_dataset = pickle.load(f)\n",
    "\n",
    "with open(f\"{data_path}/test_dataset.pkl\", \"rb\") as f:\n",
    "    test_dataset = pickle.load(f)\n",
    "\n",
    "len(train_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ff0d100",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['features', 'pressing_intensity', 'label', 'presser_id', 'agent_order'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = train_dataset[0]\n",
    "sample.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "542435ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'torch.Size([1, 11, 10])', 'torch.Size([1, 11, 11])', 'torch.Size([1, 23, 8])', 'torch.Size([1, 10, 10])', 'torch.Size([1, 10, 11])'}\n"
     ]
    }
   ],
   "source": [
    "feature_len = []\n",
    "for sample in train_dataset:\n",
    "    feature_len.append(str(sample['features'][-1:, ...].shape))\n",
    "    feature_len.append(str(sample['pressing_intensity'][-1:, ...].shape))\n",
    "print(set(feature_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11367162",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 23, 8])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample['features'][-1:, ...].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df18d5ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features : torch.Size([2, 23, 8])\n",
      "Pressing Intensity : torch.Size([2, 11, 11])\n",
      "Labels : 0\n",
      "Presser ID : 77414\n",
      "Players Order : ['188178', '250079', '250101', '250102', '500133', '500140', '500141', '500142', '62365', '62386', '77414', '187259', '343587', '408792', '500113', '500115', '500116', '500117', '500118', '500121', '500502', '83615', 'ball']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Features : {sample['features'].shape}\")\n",
    "print(f\"Pressing Intensity : {sample['pressing_intensity'].shape}\")\n",
    "print(f\"Labels : {sample['label']}\")\n",
    "print(f\"Presser ID : {sample['presser_id']}\")\n",
    "print(f\"Players Order : {sample['agent_order']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "03cccea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x : -25.148563385009766 ~ 53.016780853271484\n",
      "y : -7.853689670562744 ~ 20.099321365356445\n",
      "vx : -1.6054884195327759 ~ 5.0920281410217285\n",
      "vy : -12.45792007446289 ~ 0.8517857193946838\n",
      "v : 0.07405897229909897 ~ 13.411462783813477\n",
      "ax : -4.507462978363037 ~ 1.2211380004882812\n",
      "ay : -7.128241539001465 ~ 2.685581922531128\n",
      "a : 0.19751569628715515 ~ 6.0\n"
     ]
    }
   ],
   "source": [
    "x_tensor_lst = [sample['features'] for sample in train_dataset]\n",
    "x_tensor_lst = torch.cat(x_tensor_lst)\n",
    "feature_cols = ['x', 'y', 'vx', 'vy', 'v', 'ax', 'ay', 'a']\n",
    "\n",
    "for i in range(8):\n",
    "    print(f\"{feature_cols[i]} : {x_tensor_lst[-1, ...][..., i].min()} ~ {x_tensor_lst[-1, ...][..., i].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5bc7c6",
   "metadata": {},
   "source": [
    "# 1. XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ff7bfa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_seq = 150\n",
    "num_agents = 11\n",
    "use_pressing_intensity = False\n",
    "selected_features_idx = [1, 2, 4, 7]\n",
    "\n",
    "train_features = []\n",
    "train_labels = []\n",
    "for i in range(len(train_dataset)):\n",
    "    sample = train_dataset[i]\n",
    "    x_tensor = sample['features'][..., selected_features_idx][-90:]\n",
    "    press_intensity = sample['pressing_intensity'][-90:]\n",
    "    y_tensor = sample['label']\n",
    "    \n",
    "    # Flatten the sequence data: shape (sequence_length, num_features) -> (sequence_length*num_features,)\n",
    "    feature_vector = x_tensor.flatten().numpy()\n",
    "    if use_pressing_intensity:\n",
    "        if press_intensity.shape[1] != num_agents:\n",
    "            pad_tensor = torch.zeros(press_intensity.shape[0], num_agents-press_intensity.shape[1], press_intensity.shape[2])\n",
    "            press_intensity = torch.cat([press_intensity, pad_tensor], dim=1)\n",
    "            \n",
    "        if press_intensity.shape[2] != num_agents:\n",
    "            pad_tensor = torch.zeros(press_intensity.shape[0], press_intensity.shape[1], num_agents-press_intensity.shape[2])\n",
    "            press_intensity = torch.cat([press_intensity, pad_tensor], dim=2)\n",
    "        \n",
    "        press_vector = press_intensity.flatten().numpy()\n",
    "        # Concatenate the flattened sequence data with the pressing intensity\n",
    "        feature_vector = np.concatenate((feature_vector, press_vector))\n",
    "    train_features.append(feature_vector)\n",
    "    train_labels.append(y_tensor.item())\n",
    "\n",
    "\n",
    "test_features = []\n",
    "test_labels = []\n",
    "for i in range(len(test_dataset)):\n",
    "    sample = test_dataset[i]\n",
    "    x_tensor = sample['features'][..., selected_features_idx][-90:]\n",
    "    press_intensity = sample['pressing_intensity'][-90:]\n",
    "    y_tensor = sample['label']\n",
    "    \n",
    "    # Flatten the sequence data: shape (sequence_length, num_features) -> (sequence_length*num_features,)\n",
    "    feature_vector = x_tensor.flatten().numpy()\n",
    "    \n",
    "    if use_pressing_intensity:\n",
    "        if press_intensity.shape[1] != num_agents:\n",
    "            pad_tensor = torch.zeros(press_intensity.shape[0], num_agents-press_intensity.shape[1], press_intensity.shape[2])\n",
    "            press_intensity = torch.cat([press_intensity, pad_tensor], dim=1)\n",
    "        if press_intensity.shape[2] != num_agents:\n",
    "            pad_tensor = torch.zeros(press_intensity.shape[0], press_intensity.shape[1], num_agents-press_intensity.shape[2])\n",
    "            press_intensity = torch.cat([press_intensity, pad_tensor], dim=2)\n",
    "        \n",
    "        press_vector = press_intensity.flatten().numpy()\n",
    "        # Concatenate the flattened sequence data with the pressing intensity\n",
    "        feature_vector = np.concatenate((feature_vector, press_vector))\n",
    "    test_features.append(feature_vector)\n",
    "    test_labels.append(y_tensor.item())\n",
    "\n",
    "X_train = np.array(train_features)\n",
    "y_train = np.array(train_labels)\n",
    "\n",
    "# Split the data into training and testing sets (80% training, 20% testing)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "X_test = np.array(test_features)\n",
    "y_test = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b82432f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Dataset Distribution Summary\n",
      "========================================\n",
      "Train Set:\n",
      "  Total samples: 11392\n",
      "    Label 0:  9138 samples (80.21%)\n",
      "    Label 1:  2254 samples (19.79%)\n",
      "----------------------------------------\n",
      "Validation Set:\n",
      "  Total samples: 2849\n",
      "    Label 0:  2281 samples (80.06%)\n",
      "    Label 1:   568 samples (19.94%)\n",
      "----------------------------------------\n",
      "Test Set:\n",
      "  Total samples: 1721\n",
      "    Label 0:  1358 samples (78.91%)\n",
      "    Label 1:   363 samples (21.09%)\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def print_dataset_distribution(y_train, y_val, y_test):\n",
    "    def _print_split(name, labels):\n",
    "        total = len(labels)\n",
    "        unique, counts = np.unique(labels, return_counts=True)\n",
    "        print(f\"{name} Set:\")\n",
    "        print(f\"  Total samples: {total}\")\n",
    "        for label, count in zip(unique, counts):\n",
    "            percent = (count / total) * 100\n",
    "            print(f\"    Label {label}: {count:>5} samples ({percent:5.2f}%)\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "    print(\"\\nðŸ“Š Dataset Distribution Summary\")\n",
    "    print(\"=\" * 40)\n",
    "    _print_split(\"Train\", y_train)\n",
    "    _print_split(\"Validation\", y_val)\n",
    "    _print_split(\"Test\", y_test)\n",
    "\n",
    "print_dataset_distribution(y_train, y_val, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e48d9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create XGBoost DMatrix objects for train and test sets\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_val, label=y_val)\n",
    "\n",
    "# Set XGBoost training parameters\n",
    "params = {\n",
    "    'objective': 'binary:logistic',  # binary classification\n",
    "    'eval_metric': 'auc',            # evaluation metric: AUC\n",
    "    'max_depth': 6,                  # maximum depth of trees\n",
    "    'eta': 0.1,                      # learning rate\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "# Specify the watchlist to evaluate performance on training and test sets during training\n",
    "watchlist = [(dtrain, 'train'), (dtest, 'eval')]\n",
    "num_rounds = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "856ef086",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/work/miniconda/envs/mhl_py311/lib/python3.11/site-packages/xgboost/core.py:726: FutureWarning: Pass `evals` as keyword args.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-auc:0.72404\teval-auc:0.59755\n",
      "[1]\ttrain-auc:0.77449\teval-auc:0.61684\n",
      "[2]\ttrain-auc:0.80143\teval-auc:0.61842\n",
      "[3]\ttrain-auc:0.82563\teval-auc:0.62081\n",
      "[4]\ttrain-auc:0.83993\teval-auc:0.62739\n",
      "[5]\ttrain-auc:0.85611\teval-auc:0.63009\n",
      "[6]\ttrain-auc:0.86313\teval-auc:0.63397\n",
      "[7]\ttrain-auc:0.87409\teval-auc:0.63416\n",
      "[8]\ttrain-auc:0.88935\teval-auc:0.63792\n",
      "[9]\ttrain-auc:0.89944\teval-auc:0.63648\n",
      "[10]\ttrain-auc:0.90767\teval-auc:0.63826\n",
      "[11]\ttrain-auc:0.91626\teval-auc:0.63678\n",
      "[12]\ttrain-auc:0.92196\teval-auc:0.63293\n",
      "[13]\ttrain-auc:0.92891\teval-auc:0.63344\n",
      "[14]\ttrain-auc:0.93481\teval-auc:0.63360\n",
      "[15]\ttrain-auc:0.93753\teval-auc:0.63349\n",
      "[16]\ttrain-auc:0.94254\teval-auc:0.63322\n",
      "[17]\ttrain-auc:0.94739\teval-auc:0.63347\n",
      "[18]\ttrain-auc:0.94938\teval-auc:0.63573\n",
      "[19]\ttrain-auc:0.95371\teval-auc:0.63360\n",
      "[20]\ttrain-auc:0.95596\teval-auc:0.63550\n"
     ]
    }
   ],
   "source": [
    "# Train XGBoost model with early stopping on the evaluation set\n",
    "bst = xgb.train(params, dtrain, num_rounds, watchlist, early_stopping_rounds=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96af079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8027\n",
      "Test AUC: 0.6491\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# W/O Pressing Intensity\n",
    "# Get predictions on the test set\n",
    "y_pred = bst.predict(dtest)\n",
    "y_pred_label = (y_pred > 0.5).astype(int)\n",
    "\n",
    "# Calculate and print evaluation metrics\n",
    "accuracy = accuracy_score(y_val, y_pred_label)\n",
    "auc = roc_auc_score(y_val, y_pred)\n",
    "\n",
    "print(\"Test Accuracy: {:.4f}\".format(accuracy))\n",
    "print(\"Test AUC: {:.4f}\".format(auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c5df0ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.7989\n",
      "Test AUC: 0.6465\n"
     ]
    }
   ],
   "source": [
    "# W/O Pressing Intensity\n",
    "# Get predictions on the test set\n",
    "y_pred = bst.predict(dtest)\n",
    "y_pred_label = (y_pred > 0.5).astype(int)\n",
    "\n",
    "# Calculate and print evaluation metrics\n",
    "accuracy = accuracy_score(y_val, y_pred_label)\n",
    "auc = roc_auc_score(y_val, y_pred)\n",
    "\n",
    "print(\"Test Accuracy: {:.4f}\".format(accuracy))\n",
    "print(\"Test AUC: {:.4f}\".format(auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb1665e",
   "metadata": {},
   "source": [
    "# 2. SoccerMap / exPress Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "59cea614",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import exPressInputDataset\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9a40752c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]['pressing_intensity'].shape == (1, 11, 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d064932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from /data/MHL/pressing-intensity/train_dataset.pkl...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7795"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = exPressInputDataset(f\"{data_path}/train_dataset.pkl\")\n",
    "\n",
    "cnt = 0\n",
    "for idx in range(len(train_dataset)):\n",
    "    if train_dataset[idx]['pressing_intensity'].shape == (1, 11, 11):\n",
    "        cnt += 1\n",
    "cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ff661a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5d3fe8d4",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "each element in list of batch should be of equal size",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m feat_shape = []\n\u001b[32m      2\u001b[39m press_shape = []\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeat_shape\u001b[49m\u001b[43m.\u001b[49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mfeatures\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpress_shape\u001b[49m\u001b[43m.\u001b[49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpressing_intensity\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m    \n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/mhl_py311/lib/python3.11/site-packages/torch/utils/data/dataloader.py:708\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    705\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    706\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    707\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m708\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    709\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    710\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    711\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    712\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    713\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    714\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/mhl_py311/lib/python3.11/site-packages/torch/utils/data/dataloader.py:764\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    762\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    763\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m764\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    765\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    766\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/mhl_py311/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:55\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/mhl_py311/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:398\u001b[39m, in \u001b[36mdefault_collate\u001b[39m\u001b[34m(batch)\u001b[39m\n\u001b[32m    337\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdefault_collate\u001b[39m(batch):\n\u001b[32m    338\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    339\u001b[39m \u001b[33;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[32m    340\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    396\u001b[39m \u001b[33;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[32m    397\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m398\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/mhl_py311/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:171\u001b[39m, in \u001b[36mcollate\u001b[39m\u001b[34m(batch, collate_fn_map)\u001b[39m\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collections.abc.MutableMapping):\n\u001b[32m    166\u001b[39m     \u001b[38;5;66;03m# The mapping type may have extra properties, so we can't just\u001b[39;00m\n\u001b[32m    167\u001b[39m     \u001b[38;5;66;03m# use `type(data)(...)` to create the new mapping.\u001b[39;00m\n\u001b[32m    168\u001b[39m     \u001b[38;5;66;03m# Create a clone and update it if the mapping type is mutable.\u001b[39;00m\n\u001b[32m    169\u001b[39m     clone = copy.copy(elem)\n\u001b[32m    170\u001b[39m     clone.update(\n\u001b[32m--> \u001b[39m\u001b[32m171\u001b[39m         \u001b[43m{\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m            \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcollate_fn_map\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43melem\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m    177\u001b[39m     )\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m clone\n\u001b[32m    179\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/mhl_py311/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:172\u001b[39m, in \u001b[36m<dictcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collections.abc.MutableMapping):\n\u001b[32m    166\u001b[39m     \u001b[38;5;66;03m# The mapping type may have extra properties, so we can't just\u001b[39;00m\n\u001b[32m    167\u001b[39m     \u001b[38;5;66;03m# use `type(data)(...)` to create the new mapping.\u001b[39;00m\n\u001b[32m    168\u001b[39m     \u001b[38;5;66;03m# Create a clone and update it if the mapping type is mutable.\u001b[39;00m\n\u001b[32m    169\u001b[39m     clone = copy.copy(elem)\n\u001b[32m    170\u001b[39m     clone.update(\n\u001b[32m    171\u001b[39m         {\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m             key: \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcollate_fn_map\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    175\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m elem\n\u001b[32m    176\u001b[39m         }\n\u001b[32m    177\u001b[39m     )\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m clone\n\u001b[32m    179\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/mhl_py311/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:207\u001b[39m, in \u001b[36mcollate\u001b[39m\u001b[34m(batch, collate_fn_map)\u001b[39m\n\u001b[32m    205\u001b[39m elem_size = \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mnext\u001b[39m(it))\n\u001b[32m    206\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28mlen\u001b[39m(elem) == elem_size \u001b[38;5;28;01mfor\u001b[39;00m elem \u001b[38;5;129;01min\u001b[39;00m it):\n\u001b[32m--> \u001b[39m\u001b[32m207\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33meach element in list of batch should be of equal size\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    208\u001b[39m transposed = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(*batch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n",
      "\u001b[31mRuntimeError\u001b[39m: each element in list of batch should be of equal size"
     ]
    }
   ],
   "source": [
    "feat_shape = []\n",
    "press_shape = []\n",
    "for batch in train_loader:\n",
    "    feat_shape.append(batch['features'].shape)\n",
    "    press_shape.append(batch['pressing_intensity'].shape)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "96d976f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader\n",
    "import json\n",
    "import os\n",
    "os.chdir('/home/work/MHL/express-v2')\n",
    "import argparse # To accept checkpoint path as argument\n",
    "\n",
    "# Import project modules\n",
    "# import config  # Import static configurations\n",
    "from model import PytorchSoccerMapModel # Import Lightning model\n",
    "from datasets import PressingSequenceDataset, SoccerMapInputDataset \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "08a1fab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from /data/MHL/pressing-intensity/test_dataset.pkl...\n"
     ]
    }
   ],
   "source": [
    "pl.seed_everything(42, workers=True) # Ensure reproducibility\n",
    "\n",
    "DATA_PATH = \"/data/MHL/pressing-intensity\" # Path where pickled datasets are saved\n",
    "test_dataset = SoccerMapInputDataset(os.path.join(DATA_PATH, \"test_dataset.pkl\"))\n",
    "\n",
    "if len(test_dataset) == 0:\n",
    "    print(\"Loaded test dataset is empty. Exiting.\")\n",
    "\n",
    "# Custom collate function to handle potential None values from dataset errors\n",
    "def collate_fn_skip_none(batch):\n",
    "    batch = list(filter(lambda x: x is not None, batch))\n",
    "    if not batch: return None\n",
    "    try: return torch.utils.data.dataloader.default_collate(batch)\n",
    "    except RuntimeError: return None # Skip batch if collation error\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True,\n",
    "    # collate_fn=collate_fn_skip_none\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "45aabfe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"Train a pressing evaluation model.\")\n",
    "# parser.add_argument(\"--model_type\", type=str, default=\"soccermap\", choices=['soccermap', 'xgboost', 'exPress'], help=\"Path to the model checkpoint (.ckpt) file saved during training.\")\n",
    "# parser.add_argument(\"--root_path\", type=str, default=\"/data/MHL/pressing-intensity\", help=\"Path to the data file.\")\n",
    "parser.add_argument(\"--model_type\", type=str, default=\"soccermap\", choices=['soccermap', 'xgboost', 'exPress'], help=\"Path to the model checkpoint (.ckpt) file saved during training.\")\n",
    "parser.add_argument(\"--root_path\", type=str, default=\"/data/MHL/pressing-intensity\", help=\"Path to the data file.\")\n",
    "parser.add_argument(\"--mode\", type=str, default=\"train\", choices=['train', 'test'], help=\"Mode: 'train' or 'test'.\")\n",
    "parser.add_argument(\"--ckpt_path\", type=str, default=None, help=\"Path to checkpoint file (Required for 'test' mode).\")\n",
    "parser.add_argument(\"--params_path\", type=str, default=\"params.json\", help=\"Path to the JSON containing configurations.\")\n",
    "parser.add_argument(\"--seed\", type=int, default=42, help=\"Seed number.\")\n",
    "\n",
    "args = parser.parse_args([])\n",
    "\n",
    "args.mode = 'test'\n",
    "args.model_type = \"exPress\"\n",
    "args.ckpt_path = \"/data/MHL/pressing-intensity/checkpoints/exPress-epoch=28-val_loss=0.49.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4dcd43e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configurations loaded from params.json.\n"
     ]
    }
   ],
   "source": [
    "from components import press\n",
    "\n",
    "\n",
    "component_dict = {\n",
    "                    \"soccermap\": press.SoccerMapComponent,\n",
    "                    \"exPress\": press.exPressComponent,\n",
    "                }\n",
    "\n",
    "exp = component_dict[args.model_type](args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b193af97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mhl_py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
